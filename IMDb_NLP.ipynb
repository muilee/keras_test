{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset location url=http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def rm_tags(text):\n",
    "    # re_tag = re.compile(r\"<[^>]+>\")\n",
    "    # re_tag.sub(\"\", text)\n",
    "    return re.sub(r\"<[^>]+>\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def read_files(filetype):\n",
    "    path = \"data/aclImdb/\"\n",
    "    file_list = []\n",
    "    \n",
    "    positive_path = path + filetype + \"/pos/\"\n",
    "    for f in os.listdir(positive_path):\n",
    "        file_list.append(positive_path + f)\n",
    "    \n",
    "    negative_path = path + filetype + \"/neg/\"\n",
    "    for f in os.listdir(negative_path):\n",
    "        file_list.append(negative_path + f)\n",
    "        \n",
    "    print(\"read\", filetype, \" files:\", len(file_list))\n",
    "    \n",
    "    all_labels = ([1] * 12500 + [0] * 12500)\n",
    "    \n",
    "    all_texts = []\n",
    "    for file in file_list:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as file_input:\n",
    "            all_texts.append(rm_tags(\" \".join(file_input.readlines())))\n",
    "    \n",
    "    return all_labels, all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read train  files: 25000\n"
     ]
    }
   ],
   "source": [
    "y_train, train_text = read_files(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read test  files: 25000\n"
     ]
    }
   ],
   "source": [
    "y_test, test_text = read_files(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I\\'m a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token = Tokenizer(num_words=2000)\n",
    "token.fit_on_texts(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(token.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# token.word_docs     quantity of reviews that the word exists in\n",
    "# token.word_counts   quantity of words in all review\n",
    "# token.word_index    give a number to the word according to the quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_seq = token.texts_to_sequences(train_text)\n",
    "x_test_seq = token.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[308, 6, 3, 1068, 208, 8, 29, 1, 168, 54, 13, 45, 81, 40, 391, 109, 137, 13, 57, 149, 7, 1, 481, 68, 5, 260, 11, 6, 72, 5, 631, 70, 6, 1, 5, 1, 1530, 33, 66, 63, 204, 139, 64, 1229, 1, 4, 1, 222, 899, 28, 68, 4, 1, 9, 693, 2, 64, 1530, 50, 9, 215, 1, 386, 7, 59, 3, 1470, 798, 5, 176, 1, 391, 9, 1235, 29, 308, 3, 352, 343, 142, 129, 5, 27, 4, 125, 1470, 5, 308, 9, 532, 11, 107, 1466, 4, 57, 554, 100, 11, 308, 6, 226, 47, 3, 11, 8, 214]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
     ]
    }
   ],
   "source": [
    "print(train_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change length of every review to 100 \n",
    "\n",
    "x_train = sequence.pad_sequences(x_train_seq, maxlen=100)\n",
    "x_test = sequence.pad_sequences(x_test_seq, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 115\n",
      "after 100\n"
     ]
    }
   ],
   "source": [
    "print(\"before\", len(x_train_seq[2]))\n",
    "print(\"after\", len(x_train[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(output_dim=32, input_dim=2000, input_length=100))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=256, activation=\"relu\"))\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 32)           64000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               819456    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 883,713\n",
      "Trainable params: 883,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.4823 - acc: 0.7559 - val_loss: 0.5923 - val_acc: 0.7210\n",
      "Epoch 2/10\n",
      " - 4s - loss: 0.2701 - acc: 0.8877 - val_loss: 0.4904 - val_acc: 0.7812\n",
      "Epoch 3/10\n",
      " - 4s - loss: 0.1643 - acc: 0.9381 - val_loss: 0.6174 - val_acc: 0.7674\n",
      "Epoch 4/10\n",
      " - 4s - loss: 0.0865 - acc: 0.9710 - val_loss: 0.7340 - val_acc: 0.7702\n",
      "Epoch 5/10\n",
      " - 4s - loss: 0.0480 - acc: 0.9840 - val_loss: 1.1631 - val_acc: 0.7198\n",
      "Epoch 6/10\n",
      " - 4s - loss: 0.0333 - acc: 0.9891 - val_loss: 1.2418 - val_acc: 0.7358\n",
      "Epoch 7/10\n",
      " - 4s - loss: 0.0298 - acc: 0.9891 - val_loss: 1.1763 - val_acc: 0.7574\n",
      "Epoch 8/10\n",
      " - 4s - loss: 0.0248 - acc: 0.9917 - val_loss: 1.1043 - val_acc: 0.7790\n",
      "Epoch 9/10\n",
      " - 4s - loss: 0.0254 - acc: 0.9908 - val_loss: 1.0688 - val_acc: 0.7880\n",
      "Epoch 10/10\n",
      " - 4s - loss: 0.0235 - acc: 0.9919 - val_loss: 1.0025 - val_acc: 0.8066\n"
     ]
    }
   ],
   "source": [
    "# training model\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, batch_size=100, epochs=10, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 37us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81503999999999999"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 33us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict_classes(x_test)\n",
    "prediction[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_classes = prediction.reshape(-1)\n",
    "predict_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentimentDict = {1: \"正面\", 0:\"負面\"}\n",
    "\n",
    "def display_test_Sentiment(i):\n",
    "    print(test_text[i])\n",
    "    print(\"label: \", SentimentDict[y_test[i]], \"predict: \", SentimentDict[predict_classes[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a recreational golfer with some knowledge of the sport's history, I was pleased with Disney's sensitivity to the issues of class in golf in the early twentieth century. The movie depicted well the psychological battles that Harry Vardon fought within himself, from his childhood trauma of being evicted to his own inability to break that glass ceiling that prevents him from being accepted as an equal in English golf society. Likewise, the young Ouimet goes through his own class struggles, being a mere caddie in the eyes of the upper crust Americans who scoff at his attempts to rise above his standing. What I loved best, however, is how this theme of class is manifested in the characters of Ouimet's parents. His father is a working-class drone who sees the value of hard work but is intimidated by the upper class; his mother, however, recognizes her son's talent and desire and encourages him to pursue his dream of competing against those who think he is inferior.Finally, the golf scenes are well photographed. Although the course used in the movie was not the actual site of the historical tournament, the little liberties taken by Disney do not detract from the beauty of the film. There's one little Disney moment at the pool table; otherwise, the viewer does not really think Disney. The ending, as in \"Miracle,\" is not some Disney creation, but one that only human history could have written.\n",
      "label:  正面 predict:  負面\n"
     ]
    }
   ],
   "source": [
    "display_test_Sentiment(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "I don't usually do reviews but this film was such a huge disappointment I couldn't fight it anymore. The original movie was so good and, considering this is the exact same movie, there was really not much that could go wrong. In theory of course, because in reality the final result is just soulless. Everything feels fake. From Emma Watson's acting to the cgi and the props. I love Emma Watson but in this film she is just playing herself trying to play Belle. To be fair though, no one in the film actually manages to instill the characters with the same emotion and personality as the original except maybe Josh Gad. Luke Evans is very good but his character is written as a villain from the start, while in the original he evolves from slightly annoying to \"evil\". Which brings me to my next point: the awful writing. The film treats the audience like we're stupid and needs to explain everything verbally instead of just letting things show through the action. The characters are one dimensional and don't change as the story unfolds. Gaston is the villain. The Beast is just a misunderstood soul from the beginning despite the prologue telling us otherwise. Even his bad temper is watered down. The writers had already the script written for them, all they had to do was add a few more lines here and there and create two or three scenes that would blend in seamlessly with the original (since, I repeat, they chose to use almost word for word the 1991 script with minor changes). Well, the new dialogue feels very wooden and unnatural. The new scenes add nothing to the story and, even though the creators try to answer some questions we have from the original, in the end they create new plot holes that go unanswered. I miss the subtlety of the 1991 film in which every expression, every line and every pause added something either to the progression of the story or the characterization of the heroes without anything feeling forced. I keep mentioning the original a lot but that is because this movie has nothing new to offer really, so I can't fully separate it from the 1991 one. In the end, what annoys me the most is that the 2017 remake had great potential to become a new classic and stand on its own had it been handled a little differently and not with a rushed \"let's make some good money\" mentality. There are very few good things about this movie, one of which is the music which is simply magical and manages to convey all the emotions the actors can't. Then there is the ending (after the transformation) where there is a more realistic touch as the villagers remember their friends-relatives that work at the castle and are finally reunited. Overall, despite the enormous hype, the movie just makes the original stand out even more as a timeless film that won't be surpassed by another adaptation any time soon.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_review(input_text):\n",
    "    input_seq = token.texts_to_sequences([input_text])\n",
    "    pad_input_seq = sequence.pad_sequences(input_seq, maxlen=100)\n",
    "    predict_result = model.predict_classes(pad_input_seq)\n",
    "    print(SentimentDict[predict_result[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "正面\n"
     ]
    }
   ],
   "source": [
    "predict_review(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change the model to 3800 words\n",
    "\n",
    "token = Tokenizer(num_words=3800)\n",
    "token.fit_on_texts(train_text)\n",
    "\n",
    "x_train_seq = token.texts_to_sequences(train_text)\n",
    "x_test_seq = token.texts_to_sequences(test_text)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train_seq, maxlen=380)\n",
    "x_test = sequence.pad_sequences(x_test_seq, maxlen=380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(output_dim=32, input_dim=3800, input_length=380))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=256, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 380, 32)           121600    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 380, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12160)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               3113216   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,235,073\n",
      "Trainable params: 3,235,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      " - 15s - loss: 0.4685 - acc: 0.7647 - val_loss: 0.4073 - val_acc: 0.8280\n",
      "Epoch 2/10\n",
      " - 15s - loss: 0.1955 - acc: 0.9235 - val_loss: 0.4471 - val_acc: 0.8174\n",
      "Epoch 3/10\n",
      " - 15s - loss: 0.0760 - acc: 0.9748 - val_loss: 0.6651 - val_acc: 0.7860\n",
      "Epoch 4/10\n",
      " - 16s - loss: 0.0286 - acc: 0.9918 - val_loss: 0.9089 - val_acc: 0.7726\n",
      "Epoch 5/10\n",
      " - 15s - loss: 0.0133 - acc: 0.9966 - val_loss: 0.9777 - val_acc: 0.7892\n",
      "Epoch 6/10\n",
      " - 15s - loss: 0.0099 - acc: 0.9973 - val_loss: 0.8830 - val_acc: 0.8158\n",
      "Epoch 7/10\n",
      " - 15s - loss: 0.0110 - acc: 0.9967 - val_loss: 1.0538 - val_acc: 0.7936\n",
      "Epoch 8/10\n",
      " - 15s - loss: 0.0098 - acc: 0.9967 - val_loss: 1.0405 - val_acc: 0.8050\n",
      "Epoch 9/10\n",
      " - 15s - loss: 0.0123 - acc: 0.9958 - val_loss: 1.7423 - val_acc: 0.7228\n",
      "Epoch 10/10\n",
      " - 15s - loss: 0.0146 - acc: 0.9950 - val_loss: 1.3401 - val_acc: 0.7748\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, batch_size=100, epochs=10, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_review(input_text):\n",
    "    input_seq = token.texts_to_sequences([input_text])\n",
    "    pad_input_seq = sequence.pad_sequences(input_seq, maxlen=380)\n",
    "    predict_result = model.predict_classes(pad_input_seq)\n",
    "    print(SentimentDict[predict_result[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 123us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84360000000000002"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a RNN model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(output_dim=32, input_dim=3800, input_length=380))\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "model.add(SimpleRNN(units=16))\n",
    "\n",
    "model.add(Dense(units=256, activation=\"relu\"))\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 380, 32)           121600    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 380, 32)           0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 16)                784       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               4352      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 126,993\n",
      "Trainable params: 126,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      " - 12s - loss: 0.5302 - acc: 0.7296 - val_loss: 0.4790 - val_acc: 0.8022\n",
      "Epoch 2/10\n",
      " - 12s - loss: 0.3603 - acc: 0.8510 - val_loss: 0.5809 - val_acc: 0.7570\n",
      "Epoch 3/10\n",
      " - 12s - loss: 0.2895 - acc: 0.8831 - val_loss: 0.4606 - val_acc: 0.8198\n",
      "Epoch 4/10\n",
      " - 12s - loss: 0.2531 - acc: 0.9017 - val_loss: 0.7098 - val_acc: 0.7358\n",
      "Epoch 5/10\n",
      " - 12s - loss: 0.2055 - acc: 0.9220 - val_loss: 0.7631 - val_acc: 0.7348\n",
      "Epoch 6/10\n",
      " - 12s - loss: 0.1576 - acc: 0.9413 - val_loss: 0.7441 - val_acc: 0.7678\n",
      "Epoch 7/10\n",
      " - 12s - loss: 0.1259 - acc: 0.9553 - val_loss: 0.8328 - val_acc: 0.7696\n",
      "Epoch 8/10\n",
      " - 12s - loss: 0.1079 - acc: 0.9605 - val_loss: 0.6824 - val_acc: 0.8094\n",
      "Epoch 9/10\n",
      " - 12s - loss: 0.0898 - acc: 0.9677 - val_loss: 0.9759 - val_acc: 0.7446\n",
      "Epoch 10/10\n",
      " - 12s - loss: 0.0780 - acc: 0.9717 - val_loss: 0.8336 - val_acc: 0.7928\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, batch_size=100, epochs=10, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 8s 337us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83348"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a LSTM model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(output_dim=32, input_dim=3800, input_length=380))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(32))\n",
    "\n",
    "model.add(Dense(units=256, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 380, 32)           121600    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 380, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 138,625\n",
      "Trainable params: 138,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      " - 36s - loss: 0.4923 - acc: 0.7522 - val_loss: 0.4399 - val_acc: 0.7970\n",
      "Epoch 2/10\n",
      " - 35s - loss: 0.2763 - acc: 0.8868 - val_loss: 0.5999 - val_acc: 0.7250\n",
      "Epoch 3/10\n",
      " - 36s - loss: 0.2260 - acc: 0.9131 - val_loss: 0.4606 - val_acc: 0.8182\n",
      "Epoch 4/10\n",
      " - 35s - loss: 0.1964 - acc: 0.9257 - val_loss: 0.3680 - val_acc: 0.8396\n",
      "Epoch 5/10\n",
      " - 35s - loss: 0.1819 - acc: 0.9306 - val_loss: 0.2851 - val_acc: 0.8872\n",
      "Epoch 6/10\n",
      " - 35s - loss: 0.1630 - acc: 0.9391 - val_loss: 0.3927 - val_acc: 0.8462\n",
      "Epoch 7/10\n",
      " - 35s - loss: 0.1434 - acc: 0.9485 - val_loss: 0.4803 - val_acc: 0.8232\n",
      "Epoch 8/10\n",
      " - 35s - loss: 0.1270 - acc: 0.9521 - val_loss: 0.4938 - val_acc: 0.8318\n",
      "Epoch 9/10\n",
      " - 35s - loss: 0.1111 - acc: 0.9607 - val_loss: 0.5395 - val_acc: 0.8244\n",
      "Epoch 10/10\n",
      " - 36s - loss: 0.1136 - acc: 0.9587 - val_loss: 0.5950 - val_acc: 0.8302\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, batch_size=100, epochs=10, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 18s 717us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85568"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_text = \"\"\"Beauty and the Beast is about a deposed slave-owning aristocrat who imprisons a farm girl. She undergoes Stockholm Syndrome, identifying with her captor, then proceeds to betray her village's uprising and reinstates the slave-owning prince to power by offering her hand in marriage.\n",
    "\n",
    "Furthermore, Belle's contempt for the provincial farming community and their lack of refinement stems from vague memories she has of a more cultured upbringing in Paris. When she later is shown a vision of her childhood house and remarks \"it's so small,\" this was a moment where she could put it all together. \n",
    "\n",
    "The lack of refinement in the rural areas was due to brutal exploitation which forced unmarried women to beg in the streets. It is likely that the community's surplus resources were taken by aristocrats like the Beast, and used to fund his opulent palace. Thus, depriving the farming community of leisure time and resources for education and arts, which would have made them more sophisticated, meeting Belle's approval.\n",
    "\n",
    "It is also possible that Gaston's intense desire to marry, which caused his nefarious plot, may be linked to levée en masse, a policy that required conscription for all unmarried French men between 18 and 25. So his patriarchal demands were a direct result of state policy to benefit the aristocracy by providing soldiers to sacrifice their lives in land disputes between inbred blue blood cousins.\n",
    "\n",
    "Then, this exploitation provided a concentration of wealth and power in the city, which created the market for her father to pursue creative employment rather than farm work. This also forced them into slums, where squalor and poor public health systems lead to the spread of plague, which is met with cold indifference by the doctor, indicating lack of public health care as a source of Belle's childhood trauma. \n",
    "\n",
    "All of this exploitation and upward wealth transfer made its way back to the remote plantation of the Beast.\n",
    "\n",
    "When confronted with this inescapable logic, what does she do? She decides to take the easy way out and enjoy the life of luxury, waited hand and foot by Beast's slaves, who feed her, clothe her, sing and dance for her. A life she always felt entitled to, on part of her feeling of superiority towards her provincial neighbors.\n",
    "\n",
    "The moral of the story is, marry for money, and ignore the suffering of the poor. A terrible message for children.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 110ms/step\n",
      "負面\n"
     ]
    }
   ],
   "source": [
    "predict_review(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
